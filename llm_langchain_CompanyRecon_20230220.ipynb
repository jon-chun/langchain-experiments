{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain: LLM + Web Scraper\n",
        "\n",
        "* https://github.com/leegonzales/LangChainExamples"
      ],
      "metadata": {
        "id": "gHiWzjT24KAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "33sZwzLFRlz5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icMXn63poSKg",
        "outputId": "c3e92961-7242-4fbb-decc-cd4666745778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.92-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.8/288.8 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (2.25.1)\n",
            "Collecting openai\n",
            "  Downloading openai-0.26.5.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain) (1.21.6)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain) (1.4.46)\n",
            "Requirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.8/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain) (1.10.4)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.8/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from langchain) (8.2.1)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.0.1)\n",
            "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.26.5-py3-none-any.whl size=67620 sha256=2fe2494f9464c0b57d2f390a0fdcb6054c0bf76126d267a3e49bc0c084e56af2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/47/99/8273a59fbd59c303e8ff175416d5c1c9c03a2e83ebf7525a99\n",
            "Successfully built openai\n",
            "Installing collected packages: tokenizers, faiss-cpu, mypy-extensions, typing-inspect, marshmallow-enum, huggingface-hub, transformers, openai, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.7 faiss-cpu-1.7.3 huggingface-hub-0.12.1 langchain-0.0.92 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openai-0.26.5 tokenizers-0.13.2 transformers-4.26.1 typing-inspect-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain requests openai transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set OpenAI API Key"
      ],
      "metadata": {
        "id": "Gdqljqt1Rn4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Signup for an OpenAPI API Key at www.openai.com/api"
      ],
      "metadata": {
        "id": "4VJD5efg8MBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass('Enter your OpenAI key: ')\n",
        "# print(f'OPENAI_API_KEY is: {OPENAI_API_KEY}')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkHoYij_55yO",
        "outputId": "2dc9ea31-4249-45b9-8bdb-d82ddcc8b378"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "import requests"
      ],
      "metadata": {
        "id": "FUUgX0HCozbe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from time import sleep\n",
        "import pickle\n",
        "\n",
        "def get_page_text(url, depth=2, visited_links=None, max_links=10, cache=None, timeout=3, user_agent=None):\n",
        "    \"\"\"\n",
        "    Recursively follow links on a webpage and return a list of documents of subsequent found pages.\n",
        "    :param url: The URL of the webpage to scrape\n",
        "    :param depth: The number of levels deep to recursively follow links. Default is 2.\n",
        "    :param visited_links: A dictionary or list of links that have already been visited to prevent revisiting links\n",
        "    :param max_links: The maximum number of links to follow. Default is 50.\n",
        "    :param cache: A cache of links and their corresponding documents to prevent unnecessary web requests\n",
        "    :param timeout: Number of seconds to wait before timing out a request. Default is 5.\n",
        "    :param user_agent: The User Agent string to use for requests. Default is None.\n",
        "    \"\"\"\n",
        "    # Initialize the visited links set if not provided\n",
        "    if visited_links is None:\n",
        "        visited_links = {}\n",
        "    if cache is None:\n",
        "        cache = {}\n",
        "    # Extract the root domain from the URL\n",
        "    parsed_uri = urlparse(url)\n",
        "    root_domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
        "    # Check if the link has already been visited\n",
        "    if url in visited_links:\n",
        "        print(\"Hit in visited_links set: \", url)\n",
        "        return None\n",
        "    # Check if the link is in the cache\n",
        "    if url in cache:\n",
        "        print(\"Hit in cache set: \", url)\n",
        "        return cache[url]\n",
        "    # Check for relative paths, fragments, and mailto links\n",
        "    if not parsed_uri.netloc:\n",
        "        print(\"Invalid URL: \", url)\n",
        "        return None\n",
        "    visited_links[url] = True\n",
        "    # Send a GET request to the URL and handle common errors\n",
        "    try:\n",
        "        headers = {'User-Agent': user_agent or 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36','Accept-Language': 'en-US,en;q=0.5'}\n",
        "        print(\"Retrieving: \", url, headers)\n",
        "        page = requests.get(url, headers=headers)\n",
        "        page.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error retrieving the webpage {url}: {str(e)}\")\n",
        "        return None\n",
        "    # parse the HTML and extract the text\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "    # Add the link and its corresponding document to the cache\n",
        "    cache[url] = Document(text={text}, metadata={\"source\": url})\n",
        "    with open('scrape_cache.pickle', 'wb') as handle:\n",
        "        pickle.dump(cache, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    # Check if we have reached the maximum depth or maximum number of links to follow\n",
        "    if depth <= 0 or max_links <= 0:\n",
        "        return cache[url]\n",
        "    # Follow links on the webpage\n",
        "    links = []\n",
        "    for link in soup.find_all('a'):\n",
        "        href = link.get('href')\n",
        "        # Only follow links that are on the same root domain\n",
        "        if href and root_domain in href:\n",
        "            links.append(href)\n",
        "    # Follow the links recursively and space out the requests to avoid throttling\n",
        "    for link in links:\n",
        "        sleep(timeout)\n",
        "        doc = get_page_text(link, depth-1, visited_links, max_links-1, cache, timeout, user_agent)\n",
        "        if doc:\n",
        "            cache[link] = doc\n",
        "    return cache\n"
      ],
      "metadata": {
        "id": "tjjWZK3s-0xq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Website URLs"
      ],
      "metadata": {
        "id": "J4NYD27nRss8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sources = [\n",
        "    get_page_text(\"https://www.guildeducation.com/solutions/\", depth=0),\n",
        "    get_page_text(\"https://www.guildeducation.com/leadership/\", depth=0),\n",
        "    get_page_text(\"https://blog.guildeducation.com/\", depth=1),\n",
        "    get_page_text(\"https://www.guildeducation.com/terms/\", depth=0),\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "Ltkzbm2nMaDN",
        "outputId": "c88fe0e6-0144-4873-c83d-7c1327abb3e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving:  https://www.guildeducation.com/solutions/ {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36', 'Accept-Language': 'en-US,en;q=0.5'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9be35b04254c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m sources = [\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mget_page_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.guildeducation.com/solutions/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mget_page_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.guildeducation.com/leadership/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mget_page_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://blog.guildeducation.com/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mget_page_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.guildeducation.com/terms/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3e4bd5c2d1b3>\u001b[0m in \u001b[0;36mget_page_text\u001b[0;34m(url, depth, visited_links, max_links, cache, timeout, user_agent)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Add the link and its corresponding document to the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scrape_cache.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pydantic/main.cpython-38-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Document\npage_content\n  field required (type=value_error.missing)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_chunks = []\n",
        "splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
        "for source in sources:\n",
        "    for chunk in splitter.split_text(source.page_content):\n",
        "        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))\n",
        "\n",
        "search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "nHSNg1cfsSYe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "38298ae4-e6aa-4183-d21e-5b94de0e2d32"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8cdd823d65ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msource_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacterTextSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msource_chunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sources' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_with_sources_chain(OpenAI(temperature=0))\n",
        "\n",
        "def print_answer(question):\n",
        "    print(\n",
        "        chain(\n",
        "            {\n",
        "                \"input_documents\": search_index.similarity_search(question, k=4),\n",
        "                \"question\": question,\n",
        "            },\n",
        "            return_only_outputs=False,\n",
        "        )[\"output_text\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "6XUK7Rlwrto6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"Who are all of the VPs at BetterUP?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ_SHC5Nptmk",
        "outputId": "82eba1ae-e9d1-4df4-d2b4-81e837126a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The VPs at BetterUP are Tom Patterson (SVP, Corporate Development & Strategy), Brad McCracken (SVP, Worldwide Sales), Armen Berjikly (VP, Product), Preeya Voss (VP, Sales, North America), Duke Daehling (VP, Sales), Erik Darby (VP, Business Development), Shonna Waters, PhD (VP, Alliance Solutions), Karen Lai (VP, Field), Evelyn Kim (VP, Product Design), Dr. Christine Carter (VP, Learning Experience Design), Katie Coupe (VP, People), Allison Yost (VP, BetterUp Labs), Adam Lavezzo (VP, Revenue Operations), Cameran Hetrick (VP, Analytics), Meredith Speece (Director of Legal and Privacy), and Chanel Fanaberia (VP, Operations).\n",
            "SOURCES: https://www.betterup.com/about-us/leadership-team?hsLang=en, https://www.betterup.com/en/about-us?hsLang=en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"How many VPs are men vs women? List the woman, list the men. Emit as a markdown table\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2dkvUFapx16",
        "outputId": "f9302df5-483e-486e-e912-60769b4cc203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " There are 8 male VPs and 6 female VPs. The male VPs are Tom Patterson, Brad McCracken, Armen Berjikly, Duke Daehling, Erik Darby, Adam Lavezzo, Cameran Hetrick, and Alexi Robichaux. The female VPs are Preeya Voss, Shonna Waters, Karen Lai, Evelyn Kim, Dr. Christine Carter, Katie Coupe, Allison Yost, and Cindy Goodrich.\n",
            "\n",
            "| Male VPs | Female VPs |\n",
            "| -------- | ---------- |\n",
            "| Tom Patterson | Preeya Voss |\n",
            "| Brad McCracken | Shonna Waters |\n",
            "| Armen Berjikly | Karen Lai |\n",
            "| Duke Daehling | Evelyn Kim |\n",
            "| Erik Darby | Dr. Christine Carter |\n",
            "| Adam Lavezzo | Katie Coupe |\n",
            "| Cameran Hetrick | Allison Yost |\n",
            "| Alexi Robichaux | Cindy Goodrich |\n",
            "\n",
            "SOURCES: https://www.betterup.com/about-us/leadership-team?hsLang=en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"What are all of the ways coaching can help people?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ybi6U5PQJ70",
        "outputId": "52fedd19-680d-47ab-bbb6-155349a386a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Coaching can help people by providing support, guidance, and accountability to help them reach their goals, build resilience, and develop skills to manage stress and anxiety.\n",
            "SOURCES: https://www.betterup.com/blog/page/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"What is Better UP?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZURHlLgs5NQn",
        "outputId": "ba93f644-f284-48af-9e8a-8e6f8a89058d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " BetterUp is a coaching and Care platform that helps organizations build a happier, healthier workforce that fuels business growth. It provides world-class coaching, AI technology, and behavioral science experts to deliver change at scale, improving individual resilience, adaptability, and effectiveness.\n",
            "SOURCES: https://www.betterup.com/en/about-us?hsLang=en, https://www.betterup.com/about-us/careers\n"
          ]
        }
      ]
    }
  ]
}